{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "언어 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 글자 수 : 2701\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")\n",
    "\n",
    "data = dataset\n",
    "ko_text = \"\".join(data[\"train\"][\"document\"])\n",
    "ko_chars = sorted(list(set((ko_text))))\n",
    "ko_vocab_size = len(ko_chars)\n",
    "print(\"총 글자 수 :\", ko_vocab_size)\n",
    "\n",
    "character_to_ids = {char:i for i, char in enumerate(ko_chars)}\n",
    "ids_to_character = {i:char for i, char in enumerate(ko_chars)}\n",
    "token_encode = lambda s:[character_to_ids[c] for c in s]\n",
    "token_decode = lambda l: \"\".join([ids_to_character[i] for i in l])\n",
    "\n",
    "tokenized_data = torch.tensor(token_encode(ko_text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * len(tokenized_data))\n",
    "train_dataset = tokenized_data[:n]\n",
    "test_dataset = tokenized_data[n:]\n",
    "\n",
    "block_size = 8 # 한 번에 모델이 처리할 수 있는 글자의 수\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def batch_function(mode):\n",
    "    dataset = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    x = torch.stack([dataset[index:index+block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index+1:index+block_size+1] for index in idx])\n",
    "    return x, y\n",
    "\n",
    "example_x, example_y = batch_function(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2701])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)   # 단어를 벡터로 변환하는 테이블을 만듦\n",
    "        #\n",
    "        # nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # num_embeddings 는 임베딩을 할 단어의 총 수\n",
    "        # embedding_dim 은 각 단어를 표현할 벡터의 차원\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = semiGPT(ko_vocab_size)\n",
    "output = model(example_x, example_y)\n",
    "print(output.shape)\n",
    "\n",
    "# torch.Size([4, 8, 2701])\n",
    "# batch_sie 가 4, 시퀀스 길이가 8, 어휘 크기가 2701 임을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#에러가 발생되도록 설정한 코드\u001b[39;00m\n\u001b[1;32m      2\u001b[0m embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m embedding(torch\u001b[39m.\u001b[39;49mtensor([[\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m10\u001b[39;49m]]))\n",
      "File \u001b[0;32m~/conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    191\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    192\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    193\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx,\n\u001b[1;32m    194\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    195\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type,\n\u001b[1;32m    196\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq,\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse,\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/conda/envs/py310/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "#에러가 발생되도록 설정한 코드 ---> IndexError: index out of range in self\n",
    "embedding = nn.Embedding(4, 4)\n",
    "embedding(torch.tensor([[0, 1, 2, 10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [4, 2701], got [4, 8]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[39mreturn\u001b[39;00m logits, loss\n\u001b[1;32m     17\u001b[0m model \u001b[39m=\u001b[39m semiGPT(ko_vocab_size)\n\u001b[0;32m---> 18\u001b[0m output, loss \u001b[39m=\u001b[39m model(example_x, example_y)\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36msemiGPT.forward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs, targets):\n\u001b[1;32m     12\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_token_table(inputs)\n\u001b[0;32m---> 14\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(logits, targets)\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m logits, loss\n",
      "File \u001b[0;32m~/conda/envs/py310/lib/python3.10/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\n\u001b[1;32m   3480\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   3481\u001b[0m     target,\n\u001b[1;32m   3482\u001b[0m     weight,\n\u001b[1;32m   3483\u001b[0m     _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction),\n\u001b[1;32m   3484\u001b[0m     ignore_index,\n\u001b[1;32m   3485\u001b[0m     label_smoothing,\n\u001b[1;32m   3486\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [4, 2701], got [4, 8]"
     ]
    }
   ],
   "source": [
    "#에러가 발생되도록 세팅된 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "\n",
    "        # loss 함수에 cross entropy 사용 설정\n",
    "        # 모델은 target size [4, 2701] 을 기대했는데, 실제 targets 는 [4, 8] 을 받았기 때문이다.\n",
    "\n",
    "        # shape 조정해야 함\n",
    "        # logits 는 [4, 8, 2701] 이므로 이를 [32, 2701] 로 바꾸고\n",
    "        # targets 는 [4, 8] 에서 [32] 로 변경해야 한다.\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "model = semiGPT(ko_vocab_size)\n",
    "output, loss = model(example_x, example_y)  # -> RuntimeError: Expected target size [4, 2701], got [4, 8]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits의 shape는 :  torch.Size([32, 2701]) 입니다.\n",
      "targets의 shape는 :  torch.Size([32]) 입니다.\n",
      "tensor(8.2693, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# shape 변환된 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "\n",
    "        batch, seq_length, vocab_length = logits.shape\n",
    "        logits = logits.view(batch * seq_length, vocab_length)\n",
    "\n",
    "        targets = targets.view(batch*seq_length)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        print(\"logits의 shape는 : \", logits.shape, \"입니다.\")\n",
    "        print(\"targets의 shape는 : \", targets.shape, \"입니다.\")\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "model = semiGPT(ko_vocab_size)\n",
    "logits, loss = model(example_x, example_y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_x.shape, example_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate 메서드 추가\n",
    "\n",
    "학습한 모델이 예측한 글자를 생성하기 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.4701, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 鋪좇異e₊굿比끄족公'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_length, vocab_length = logits.shape\n",
    "            logits = logits.view(batch * seq_length, vocab_length)\n",
    "            targets = targets.view(batch*seq_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(inputs)\n",
    "            logits = logits[:, -1, :]\n",
    "            print(logits.shape)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1)\n",
    "            inputs = torch.cat((inputs, next_inputs), dim=1)\n",
    "        return inputs\n",
    "\n",
    "model = semiGPT(ko_vocab_size)\n",
    "logits, loss = model(example_x, example_y)\n",
    "print(loss)\n",
    "\n",
    "token_decode(model.generate(torch.zeros((1,1),\n",
    "                                        dtype=torch.long),\n",
    "                            max_new_tokens=10)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택되는 값        :  tensor([[0.3000, 0.4000, 0.1000, 0.2000]])\n",
      "결과에 대한 size 값 :  torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "logits = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.1, 0.2, 0.3, 0.4],\n",
    "            [0.2, 0.3, 0.4, 0.1],\n",
    "            [0.3, 0.4, 0.1, 0.2]\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = logits[:,-1,:]\n",
    "print(\"선택되는 값        : \", result)\n",
    "print(\"결과에 대한 size 값 : \", result.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 사용하도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "tensor(8.3893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 뗍둔異프숩뷔튬꺼乎혐'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length).to(device)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        inputs = inputs.to(device)\n",
    "        if targets is not None:\n",
    "            targets = targets.to(device)\n",
    "        \n",
    "        logits = self.embedding_token_table(inputs)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_length, vocab_length = logits.shape\n",
    "            logits = logits.view(batch * seq_length, vocab_length)\n",
    "            targets = targets.view(batch*seq_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        inputs = inputs.to(device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(inputs)\n",
    "            logits = logits[:, -1, :]\n",
    "            print(logits.shape)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1).to(device)\n",
    "            inputs = torch.cat((inputs, next_inputs), dim=1)\n",
    "        return inputs\n",
    "\n",
    "model = semiGPT(ko_vocab_size)\n",
    "logits, loss = model(example_x, example_y)\n",
    "print(loss)\n",
    "\n",
    "token_decode(model.generate(torch.zeros((1,1),\n",
    "                                        dtype=torch.long),\n",
    "                            max_new_tokens=10)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer 추가하기\n",
    "\n",
    "모델 훈련 시 손실 함수를 이용해 모델의 예측 값과 실제 정답 데이터 사이의 차이(손실)를 계산하고  \n",
    "손실을 최소화하기 위해 모델의 매개변수를 적절히 조정한다.\n",
    "\n",
    "옵티마이저는 이 매개변수 조정 과정을 담당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "model = semiGPT(ko_vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:13<00:00, 752.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1945948600769043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "for steps in tqdm(range(10000)):\n",
    "    example_x, example_y = batch_function(\"train\")\n",
    "    example_x = example_x.to(device)\n",
    "    example_y = example_y.to(device)\n",
    "    logits, loss = model(example_x, example_y)\n",
    "    # 옵티마이저 초기화 \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # 역전파 계산 \n",
    "    loss.backward()\n",
    "    # 가중치 업데이트 \n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      "torch.Size([1, 2701])\n",
      " 퍼니다. 등급증스에\n"
     ]
    }
   ],
   "source": [
    "print(token_decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.no_grad() 데코렝이터에 의해 해당 함수 내에서 이뤄지는 모든 연산에 대해 그레디언트 계산을 자동으로 비활성화한다.\n",
    "# 중간중간 평가하는 함수용\n",
    "# 모델을 평가하는 단계에서는 그레디언트 계산과 가중치 업데이트가 필요 없다.\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_loss_metrics():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for mode in [\"train\", \"eval\"]:\n",
    "        losses = torch.zeros(eval_iteration)\n",
    "        for k in range(eval_iteration):\n",
    "            inputs, targets = batch_function(mode)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[mode] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
