{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 글자 수 : 2701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fab9e1af7f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")\n",
    "\n",
    "data = dataset\n",
    "ko_text = \"\".join(data[\"train\"][\"document\"])\n",
    "ko_chars = sorted(list(set((ko_text))))\n",
    "ko_vocab_size = len(ko_chars)\n",
    "print(\"총 글자 수 :\", ko_vocab_size)\n",
    "\n",
    "character_to_ids = {char:i for i, char in enumerate(ko_chars)}\n",
    "ids_to_character = {i:char for i, char in enumerate(ko_chars)}\n",
    "token_encode = lambda s:[character_to_ids[c] for c in s]\n",
    "token_decode = lambda l: \"\".join([ids_to_character[i] for i in l])\n",
    "\n",
    "tokenized_data = torch.tensor(token_encode(ko_text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * len(tokenized_data))\n",
    "train_dataset = tokenized_data[:n]\n",
    "test_dataset = tokenized_data[n:]\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, train loss : 8.3552, val loss : 8.3545\n",
      "step : 300, train loss : 6.0840, val loss : 6.0975\n",
      "step : 600, train loss : 4.7668, val loss : 4.7639\n",
      "step : 900, train loss : 4.2182, val loss : 4.2527\n",
      "step : 1200, train loss : 3.9751, val loss : 3.9498\n",
      "step : 1500, train loss : 3.8110, val loss : 3.8238\n",
      "step : 1800, train loss : 3.7284, val loss : 3.7125\n",
      "step : 2100, train loss : 3.6604, val loss : 3.6491\n",
      "step : 2400, train loss : 3.6326, val loss : 3.5933\n",
      "step : 2700, train loss : 3.5815, val loss : 3.5987\n",
      "step : 3000, train loss : 3.5630, val loss : 3.5603\n",
      "step : 3300, train loss : 3.5380, val loss : 3.5452\n",
      "step : 3600, train loss : 3.5020, val loss : 3.5332\n",
      "step : 3900, train loss : 3.5006, val loss : 3.4992\n",
      "step : 4200, train loss : 3.4940, val loss : 3.4854\n",
      "step : 4500, train loss : 3.4734, val loss : 3.4746\n",
      "step : 4800, train loss : 3.4622, val loss : 3.4616\n",
      "step : 5100, train loss : 3.4861, val loss : 3.4637\n",
      "step : 5400, train loss : 3.4492, val loss : 3.4620\n",
      "step : 5700, train loss : 3.4475, val loss : 3.4574\n",
      "step : 6000, train loss : 3.4422, val loss : 3.4626\n",
      "step : 6300, train loss : 3.4502, val loss : 3.4567\n",
      "step : 6600, train loss : 3.4368, val loss : 3.4559\n",
      "step : 6900, train loss : 3.4471, val loss : 3.4477\n",
      "step : 7200, train loss : 3.4256, val loss : 3.4263\n",
      "step : 7500, train loss : 3.4238, val loss : 3.4338\n",
      "step : 7800, train loss : 3.4211, val loss : 3.4238\n",
      "step : 8100, train loss : 3.4324, val loss : 3.4203\n",
      "step : 8400, train loss : 3.4240, val loss : 3.4294\n",
      "step : 8700, train loss : 3.4289, val loss : 3.4119\n",
      "step : 9000, train loss : 3.4032, val loss : 3.4353\n",
      "step : 9300, train loss : 3.4161, val loss : 3.4256\n",
      "step : 9600, train loss : 3.4162, val loss : 3.4276\n",
      "step : 9900, train loss : 3.4049, val loss : 3.4106\n",
      "step : 10200, train loss : 3.4094, val loss : 3.4042\n",
      "step : 10500, train loss : 3.4157, val loss : 3.4047\n",
      "step : 10800, train loss : 3.4155, val loss : 3.4149\n",
      "step : 11100, train loss : 3.4049, val loss : 3.4275\n",
      "step : 11400, train loss : 3.4118, val loss : 3.4145\n",
      "step : 11700, train loss : 3.3977, val loss : 3.4151\n",
      "step : 12000, train loss : 3.3948, val loss : 3.4215\n",
      "step : 12300, train loss : 3.4114, val loss : 3.4154\n",
      "step : 12600, train loss : 3.4039, val loss : 3.3968\n",
      "step : 12900, train loss : 3.3930, val loss : 3.4303\n",
      "step : 13200, train loss : 3.3836, val loss : 3.4117\n",
      "step : 13500, train loss : 3.4075, val loss : 3.4116\n",
      "step : 13800, train loss : 3.3879, val loss : 3.4039\n",
      "step : 14100, train loss : 3.4068, val loss : 3.4130\n",
      "step : 14400, train loss : 3.4050, val loss : 3.4008\n",
      "step : 14700, train loss : 3.4012, val loss : 3.4134\n",
      "step : 15000, train loss : 3.4043, val loss : 3.4191\n",
      "step : 15300, train loss : 3.4084, val loss : 3.4178\n",
      "step : 15600, train loss : 3.3843, val loss : 3.4207\n",
      "step : 15900, train loss : 3.4013, val loss : 3.3927\n",
      "step : 16200, train loss : 3.3892, val loss : 3.4058\n",
      "step : 16500, train loss : 3.3929, val loss : 3.4045\n",
      "step : 16800, train loss : 3.3960, val loss : 3.4256\n",
      "step : 17100, train loss : 3.3921, val loss : 3.4151\n",
      "step : 17400, train loss : 3.4014, val loss : 3.4017\n",
      "step : 17700, train loss : 3.4053, val loss : 3.4002\n",
      "step : 18000, train loss : 3.3915, val loss : 3.3961\n",
      "step : 18300, train loss : 3.3839, val loss : 3.3928\n",
      "step : 18600, train loss : 3.4001, val loss : 3.4046\n",
      "step : 18900, train loss : 3.4159, val loss : 3.3787\n",
      "step : 19200, train loss : 3.3801, val loss : 3.4023\n",
      "step : 19500, train loss : 3.3973, val loss : 3.3985\n",
      "step : 19800, train loss : 3.3915, val loss : 3.4035\n",
      "step : 20100, train loss : 3.3956, val loss : 3.4026\n",
      "step : 20400, train loss : 3.3924, val loss : 3.3955\n",
      "step : 20700, train loss : 3.4051, val loss : 3.3911\n",
      "step : 21000, train loss : 3.3776, val loss : 3.4120\n",
      "step : 21300, train loss : 3.3952, val loss : 3.4037\n",
      "step : 21600, train loss : 3.3850, val loss : 3.3856\n",
      "step : 21900, train loss : 3.3852, val loss : 3.3983\n",
      "step : 22200, train loss : 3.3840, val loss : 3.4007\n",
      "step : 22500, train loss : 3.3976, val loss : 3.3950\n",
      "step : 22800, train loss : 3.3865, val loss : 3.4145\n",
      "step : 23100, train loss : 3.3974, val loss : 3.3912\n",
      "step : 23400, train loss : 3.4040, val loss : 3.4166\n",
      "step : 23700, train loss : 3.3935, val loss : 3.4041\n",
      "step : 24000, train loss : 3.3965, val loss : 3.3984\n",
      "step : 24300, train loss : 3.4036, val loss : 3.3995\n",
      "step : 24600, train loss : 3.4132, val loss : 3.3928\n",
      "step : 24900, train loss : 3.3873, val loss : 3.3915\n",
      "step : 25200, train loss : 3.3931, val loss : 3.4035\n",
      "step : 25500, train loss : 3.3987, val loss : 3.4005\n",
      "step : 25800, train loss : 3.3822, val loss : 3.3968\n",
      "step : 26100, train loss : 3.3879, val loss : 3.3837\n",
      "step : 26400, train loss : 3.3894, val loss : 3.4027\n",
      "step : 26700, train loss : 3.3866, val loss : 3.3799\n",
      "step : 27000, train loss : 3.3841, val loss : 3.4161\n",
      "step : 27300, train loss : 3.3918, val loss : 3.3969\n",
      "step : 27600, train loss : 3.4010, val loss : 3.3922\n",
      "step : 27900, train loss : 3.3814, val loss : 3.3926\n",
      "step : 28200, train loss : 3.3960, val loss : 3.4253\n",
      "step : 28500, train loss : 3.3824, val loss : 3.4249\n",
      "step : 28800, train loss : 3.3924, val loss : 3.3953\n",
      "step : 29100, train loss : 3.3989, val loss : 3.3934\n",
      "step : 29400, train loss : 3.3933, val loss : 3.3999\n",
      "step : 29700, train loss : 3.3924, val loss : 3.3881\n",
      "step : 30000, train loss : 3.4032, val loss : 3.3911\n",
      "step : 30300, train loss : 3.3888, val loss : 3.3903\n",
      "step : 30600, train loss : 3.3867, val loss : 3.4128\n",
      "step : 30900, train loss : 3.3940, val loss : 3.3839\n",
      "step : 31200, train loss : 3.3871, val loss : 3.3921\n",
      "step : 31500, train loss : 3.4005, val loss : 3.3779\n",
      "step : 31800, train loss : 3.3932, val loss : 3.4077\n",
      "step : 32100, train loss : 3.3926, val loss : 3.4054\n",
      "step : 32400, train loss : 3.3823, val loss : 3.4064\n",
      "step : 32700, train loss : 3.3908, val loss : 3.4006\n",
      "step : 33000, train loss : 3.3983, val loss : 3.4079\n",
      "step : 33300, train loss : 3.3789, val loss : 3.3955\n",
      "step : 33600, train loss : 3.3944, val loss : 3.3953\n",
      "step : 33900, train loss : 3.4064, val loss : 3.4006\n",
      "step : 34200, train loss : 3.3898, val loss : 3.3959\n",
      "step : 34500, train loss : 3.3831, val loss : 3.3906\n",
      "step : 34800, train loss : 3.4012, val loss : 3.3846\n",
      "step : 35100, train loss : 3.3938, val loss : 3.4088\n",
      "step : 35400, train loss : 3.3921, val loss : 3.4056\n",
      "step : 35700, train loss : 3.3864, val loss : 3.4024\n",
      "step : 36000, train loss : 3.3831, val loss : 3.3969\n",
      "step : 36300, train loss : 3.3895, val loss : 3.3992\n",
      "step : 36600, train loss : 3.3820, val loss : 3.3980\n",
      "step : 36900, train loss : 3.3872, val loss : 3.3972\n",
      "step : 37200, train loss : 3.3834, val loss : 3.4062\n",
      "step : 37500, train loss : 3.3933, val loss : 3.3923\n",
      "step : 37800, train loss : 3.3918, val loss : 3.3946\n",
      "step : 38100, train loss : 3.3861, val loss : 3.4035\n",
      "step : 38400, train loss : 3.3916, val loss : 3.4150\n",
      "step : 38700, train loss : 3.3895, val loss : 3.4092\n",
      "step : 39000, train loss : 3.3932, val loss : 3.3917\n",
      "step : 39300, train loss : 3.3820, val loss : 3.4006\n",
      "step : 39600, train loss : 3.4056, val loss : 3.4093\n",
      "step : 39900, train loss : 3.3880, val loss : 3.3966\n",
      "step : 40200, train loss : 3.3839, val loss : 3.3906\n",
      "step : 40500, train loss : 3.3975, val loss : 3.3837\n",
      "step : 40800, train loss : 3.4016, val loss : 3.3980\n",
      "step : 41100, train loss : 3.3980, val loss : 3.4076\n",
      "step : 41400, train loss : 3.4070, val loss : 3.4078\n",
      "step : 41700, train loss : 3.3838, val loss : 3.3920\n",
      "step : 42000, train loss : 3.4021, val loss : 3.3887\n",
      "step : 42300, train loss : 3.4045, val loss : 3.4089\n",
      "step : 42600, train loss : 3.3723, val loss : 3.4148\n",
      "step : 42900, train loss : 3.3970, val loss : 3.4092\n",
      "step : 43200, train loss : 3.3942, val loss : 3.4195\n",
      "step : 43500, train loss : 3.4011, val loss : 3.4012\n",
      "step : 43800, train loss : 3.3941, val loss : 3.4082\n",
      "step : 44100, train loss : 3.3951, val loss : 3.4017\n",
      "step : 44400, train loss : 3.4010, val loss : 3.4040\n",
      "step : 44700, train loss : 3.3847, val loss : 3.4157\n",
      "step : 45000, train loss : 3.3896, val loss : 3.3889\n",
      "step : 45300, train loss : 3.3983, val loss : 3.3958\n",
      "step : 45600, train loss : 3.4023, val loss : 3.3998\n",
      "step : 45900, train loss : 3.3937, val loss : 3.4036\n",
      "step : 46200, train loss : 3.3823, val loss : 3.4013\n",
      "step : 46500, train loss : 3.3897, val loss : 3.4177\n",
      "step : 46800, train loss : 3.3869, val loss : 3.3872\n",
      "step : 47100, train loss : 3.3740, val loss : 3.4069\n",
      "step : 47400, train loss : 3.3869, val loss : 3.3854\n",
      "step : 47700, train loss : 3.3953, val loss : 3.3981\n",
      "step : 48000, train loss : 3.3928, val loss : 3.4154\n",
      "step : 48300, train loss : 3.3930, val loss : 3.4041\n",
      "step : 48600, train loss : 3.3948, val loss : 3.4054\n",
      "step : 48900, train loss : 3.4009, val loss : 3.3996\n",
      "step : 49200, train loss : 3.4130, val loss : 3.4057\n",
      "step : 49500, train loss : 3.3858, val loss : 3.3963\n",
      "step : 49800, train loss : 3.3875, val loss : 3.3897\n",
      " 2개인프리 것을 시장했다립한 기록임· SM특허등급여파업부장과의한편리해서 가장원 최종 CO 될 비전 예측정이버 개월 2위 패션보유수의 초 같아지표준비스1만원자는 감한다 차단편 국민의\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iteration = 50000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iteration = 200\n",
    "\n",
    "def batch_function(mode):\n",
    "    dataset = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    x = torch.stack([dataset[index:index+block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index+1:index+block_size+1] for index in idx])\n",
    "    x, y = x.to(device), y.to(device) # .to 를 추가\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_loss_metrics():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for mode in [\"train\", \"eval\"]:\n",
    "        losses = torch.zeros(eval_iteration)\n",
    "        for k in range(eval_iteration):\n",
    "            inputs, targets = batch_function(mode)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[mode] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_length, vocab_length = logits.shape\n",
    "            logits = logits.view(batch * seq_length, vocab_length)\n",
    "            targets = targets.view(batch*seq_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(inputs)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1)\n",
    "            inputs = torch.cat((inputs, next_inputs), dim=1)\n",
    "        return inputs\n",
    "\n",
    "model = semiGPT(ko_vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iteration):\n",
    "    if step % eval_interval == 0 :\n",
    "        losses = compute_loss_metrics()\n",
    "        print(f'step : {step}, train loss : {losses[\"train\"]:.4f}, val loss : {losses[\"eval\"]:.4f}')\n",
    "\n",
    "    example_x, example_y = batch_function(\"train\")\n",
    "    logits, loss = model(example_x, example_y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "inputs = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(token_decode(model.generate(inputs, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
