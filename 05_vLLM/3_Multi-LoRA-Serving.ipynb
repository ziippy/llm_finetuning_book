{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haiqv/conda/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 59388.38it/s]\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 23734.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "lora_adapter1 = \"daje/chapter5_psychological_chatbots\"\n",
    "lora_adapter1_path = snapshot_download(repo_id=lora_adapter1)\n",
    "lora_adapter2 = \"daje/chapter5_code-llama3-8B-text-to-sql-ver0.1\"\n",
    "lora_adapter2_path = snapshot_download(repo_id=lora_adapter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-03 12:32:27 api_server.py:712] vLLM API server version 0.6.6.post1\n",
      "INFO 01-03 12:32:27 api_server.py:713] args: Namespace(subparser='serve', model_tag='allganize/Llama-3-Alpha-Ko-8B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='lora_adapter1', path='/home/haiqv/.cache/huggingface/hub/models--daje--chapter5_psychological_chatbots/snapshots/ccc9ec380559b107b0ff43180a196093d1e0e47e', base_model_name=None), LoRAModulePath(name='lora_adapter2', path='/home/haiqv/.cache/huggingface/hub/models--daje--chapter5_code-llama3-8B-text-to-sql-ver0.1/snapshots/dfa935ef610806c0b0207238a638dcbd50a68e9a', base_model_name=None)], prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='allganize/Llama-3-Alpha-Ko-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=256, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f5297155510>)\n",
      "INFO 01-03 12:32:27 api_server.py:199] Started engine process with PID 1148733\n",
      "INFO 01-03 12:32:36 config.py:510] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 01-03 12:32:40 config.py:510] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-03 12:32:40 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='allganize/Llama-3-Alpha-Ko-8B-Instruct', speculative_config=None, tokenizer='allganize/Llama-3-Alpha-Ko-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allganize/Llama-3-Alpha-Ko-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n",
      "INFO 01-03 12:32:42 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-03 12:32:43 model_runner.py:1094] Starting to load model allganize/Llama-3-Alpha-Ko-8B-Instruct...\n",
      "INFO 01-03 12:32:44 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.68s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.71s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.18s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.41s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.43s/it]\n",
      "\n",
      "INFO 01-03 12:32:51 model_runner.py:1099] Loading model weights took 14.9634 GB\n",
      "INFO 01-03 12:32:51 punica_selector.py:11] Using PunicaWrapperGPU.\n",
      "INFO 01-03 12:32:56 worker.py:241] Memory profiling takes 5.23 seconds\n",
      "INFO 01-03 12:32:56 worker.py:241] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB\n",
      "INFO 01-03 12:32:56 worker.py:241] model weights take 14.96GiB; non_torch_memory takes 1.51GiB; PyTorch activation peak memory takes 1.24GiB; the rest of the memory reserved for KV Cache is 17.75GiB.\n",
      "INFO 01-03 12:32:56 gpu_executor.py:76] # GPU blocks: 9085, # CPU blocks: 2048\n",
      "INFO 01-03 12:32:56 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 17.74x\n",
      "INFO 01-03 12:33:05 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:23<00:00,  1.50it/s]\n",
      "INFO 01-03 12:33:28 model_runner.py:1535] Graph capturing finished in 24 secs, took 2.37 GiB\n",
      "INFO 01-03 12:33:28 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 37.66 seconds\n",
      "INFO 01-03 12:33:29 api_server.py:640] Using supplied chat template:\n",
      "INFO 01-03 12:33:29 api_server.py:640] None\n",
      "INFO 01-03 12:33:29 launcher.py:19] Available routes are:\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /pooling, Methods: POST\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /score, Methods: POST\n",
      "INFO 01-03 12:33:29 launcher.py:27] Route: /v1/score, Methods: POST\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1148612\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "INFO 01-03 12:34:08 logger.py:37] Received request cmpl-b66f5436ef0347dcb16184a8c482fd6e-0: prompt: '오늘 너무 힘들어요!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.9, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 58368, 105622, 109748, 110671, 65950, 105807, 0], lora_request: LoRARequest(lora_name='lora_adapter1', lora_int_id=1, lora_path='/home/haiqv/.cache/huggingface/hub/models--daje--chapter5_psychological_chatbots/snapshots/ccc9ec380559b107b0ff43180a196093d1e0e47e', lora_local_path=None, long_lora_max_len=None, base_model_name='allganize/Llama-3-Alpha-Ko-8B-Instruct'), prompt_adapter_request: None.\n",
      "INFO 01-03 12:34:08 engine.py:267] Added request cmpl-b66f5436ef0347dcb16184a8c482fd6e-0.\n",
      "INFO 01-03 12:34:17 metrics.py:467] Avg prompt throughput: 0.4 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:40186 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-03 12:34:28 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-03 12:34:38 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-03 12:35:01 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "INFO 01-03 12:35:01 logger.py:37] Received request chatcmpl-e1f114df93384687bc60c29a24bef5cd: prompt: '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n오늘 너무 힘든 하루였어요 ㅠㅠ<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.9, top_k=-1, min_p=0.0, seed=None, stop=['<|eot_id|>', 'Human:', 'Assistant:'], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=500, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: LoRARequest(lora_name='lora_adapter1', lora_int_id=1, lora_path='/home/haiqv/.cache/huggingface/hub/models--daje--chapter5_psychological_chatbots/snapshots/ccc9ec380559b107b0ff43180a196093d1e0e47e', lora_local_path=None, long_lora_max_len=None, base_model_name='allganize/Llama-3-Alpha-Ko-8B-Instruct'), prompt_adapter_request: None.\n",
      "INFO 01-03 12:35:01 engine.py:267] Added request chatcmpl-e1f114df93384687bc60c29a24bef5cd.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:49426 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-03 12:35:10 logger.py:37] Received request chatcmpl-3952c9c94d2c44b7b398edd22e9e3050: prompt: '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n오늘 너무 힘든 하루였어요 ㅠㅠ<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.9, top_k=-1, min_p=0.0, seed=None, stop=['<|eot_id|>', 'Human:', 'Assistant:'], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=500, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: LoRARequest(lora_name='lora_adapter1', lora_int_id=1, lora_path='/home/haiqv/.cache/huggingface/hub/models--daje--chapter5_psychological_chatbots/snapshots/ccc9ec380559b107b0ff43180a196093d1e0e47e', lora_local_path=None, long_lora_max_len=None, base_model_name='allganize/Llama-3-Alpha-Ko-8B-Instruct'), prompt_adapter_request: None.\n",
      "INFO 01-03 12:35:10 engine.py:267] Added request chatcmpl-3952c9c94d2c44b7b398edd22e9e3050.\n",
      "INFO 01-03 12:35:10 metrics.py:467] Avg prompt throughput: 3.5 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:38104 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-03 12:35:20 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "INFO 01-03 12:35:20 logger.py:37] Received request chatcmpl-9d262367ef67496ba28d07cb339c714c: prompt: '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nTask: tf103 차대의 총 포인트 수를 알려주세요.\\nSQL table: CREATE TABLE table_11482 (\\n    \"Year\" real,\\n    \"Chassis\" text,\\n    \"Engine\" text,\\n    \"Tyres\" text,\\n    \"Points\" real\\n)\\nSQL query:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[';\\n', '-- '], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=500, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: LoRARequest(lora_name='lora_adapter2', lora_int_id=2, lora_path='/home/haiqv/.cache/huggingface/hub/models--daje--chapter5_code-llama3-8B-text-to-sql-ver0.1/snapshots/dfa935ef610806c0b0207238a638dcbd50a68e9a', lora_local_path=None, long_lora_max_len=None, base_model_name='allganize/Llama-3-Alpha-Ko-8B-Instruct'), prompt_adapter_request: None.\n",
      "INFO 01-03 12:35:21 engine.py:267] Added request chatcmpl-9d262367ef67496ba28d07cb339c714c.\n",
      "INFO 01-03 12:35:23 metrics.py:467] Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 0.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:41338 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-03 12:35:33 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.8 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-03 12:35:43 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "!vllm serve allganize/Llama-3-Alpha-Ko-8B-Instruct \\\n",
    "    --enable-lora \\\n",
    "    --lora-modules \\\n",
    "    lora_adapter1={lora_adapter1_path} \\\n",
    "    lora_adapter2={lora_adapter2_path} \\\n",
    "    --max-lora-rank 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
