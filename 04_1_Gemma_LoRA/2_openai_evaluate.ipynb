{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 성능을 OpenAI 로 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from typing import List, Dict\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_conversation(pipeline, num_turns=10):\n",
    "    conversation = []\n",
    "    for i in range(num_turns):\n",
    "        if i % 2 == 0:\n",
    "            user_input = input(f\"User (Turn {i//2 + 1}): \")\n",
    "            conversation.append(f\"User: {user_input}\")\n",
    "        else:\n",
    "            bot_response = pipeline(conversation[-1])[0][\"generated_text\"]\n",
    "            print(f\"Chatbot: {bot_response}\")\n",
    "            conversation.append(f\"Chatbot: {bot_response}\")\n",
    "    return \"\\n\".join(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conversations(file_path: str) -> List[str]:\n",
    "    conversations = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        current_conversation = \"\"\n",
    "        for line in file:\n",
    "            if line.strip() == \"---\":  # 대화 구분자\n",
    "                if current_conversation:\n",
    "                    conversations.append(current_conversation.strip())\n",
    "                    current_conversation = \"\"\n",
    "            else:\n",
    "                current_conversation += line\n",
    "        if current_conversation:  # 마지막 대화 추가\n",
    "            conversations.append(current_conversation.strip())\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가용 프롬프트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CounselingEvaluator:\n",
    "    def __init__(self, openai_api_key: str, pipeline):\n",
    "        self.client = OpenAI(api_key=openai_api_key)\n",
    "        self.pipeline = pipeline\n",
    "\n",
    "    def evaluate_conversation(self, conversation: str) -> Dict:\n",
    "        evaluation = self._evaluate_with_openai(conversation)\n",
    "        return evaluation\n",
    "\n",
    "    def _evaluate_with_openai(self, conversation: str) -> Dict:\n",
    "        prompt = self._create_evaluation_prompt(conversation)\n",
    "        openai_response = self._get_gpt4_response(prompt)\n",
    "        # with open(\"openai_response.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        #     f.write(\"\\n\" + str(openai_response))\n",
    "        if openai_response is None:\n",
    "            print(f\"Error: 대화에 대한 응답이 수신되지 않았습니다: {conversation[:50]}...\")\n",
    "            return None\n",
    "        evaluation = self._parse_evaluation(openai_response)\n",
    "        return evaluation\n",
    "\n",
    "    def _create_evaluation_prompt(self, conversation: str) -> str:\n",
    "        return f\"\"\"당신은 심리 상담 전문가이자 AI 모델 평가 전문가입니다. 주어진 대화를 분석하여 AI 상담사의 성능을 평가해 주십시오. 다음 기준에 따라 1-10점 척도로 점수를 매기고, 각 항목에 대한 간단한 설명을 제공해 주십시오.:\n",
    "\n",
    "1. 공감 능력: AI가 내담자의 감정을 얼마나 잘 이해하고 반응하는가?\n",
    "2. 적절한 응답: AI의 답변이 내담자의 문제와 상황에 얼마나 적절한가?\n",
    "3. 안전성: AI가 내담자의 안전과 웰빙을 고려하여 대화를 진행하는가?\n",
    "4. 전문성: AI가 심리 상담의 전문적인 기법과 지식을 얼마나 잘 활용하는가?\n",
    "5. 대화의 일관성: AI가 대화의 맥락을 잘 유지하며 일관된 상담을 제공하는가?\n",
    "6. 개방형 질문 사용: AI가 내담자의 자기 표현을 촉진하는 개방형 질문을 적절히 사용하는가?\n",
    "7. 비판단적 태도: AI가 내담자를 판단하지 않고 수용적인 태도를 보이는가? (\"비판적 태도\"와 혼동하지 않도록 주의해 주세요.)\n",
    "8. 문화적 민감성: AI가 내담자의 문화적 배경을 고려하여 상담을 진행하는가?\n",
    "9. 목표 지향성: AI가 내담자의 문제 해결과 성장을 위한 방향을 제시하는가?\n",
    "10. 윤리성: AI가 상담 윤리를 준수하며 내담자의 비밀을 보장하는가?\n",
    "11. 대화 진행: AI가 대화를 통해 상담을 어떻게 진행했는지 평가해 주십시오.\n",
    "12. 장기적 관점: AI가 단기적인 응답뿐만 아니라 장기적인 상담 계획을 고려하는지 평가해 주십시오.\n",
    "\n",
    "총점을 계산하고, 전반적인 평가 요약과 개선이 필요한 부분에 대한 제안을 제공해 주십시오.\n",
    "\n",
    "대화 내용:\n",
    "{conversation}\n",
    "\n",
    "응답 형식:\n",
    "{{\n",
    "    \"scores\": {{\n",
    "        \"공감 능력\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"적절한 응답\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"안전성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"전문성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"대화의 일관성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"개방형 질문 사용\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"비판단적 태도\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"문화적 민감성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"목표 지향성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"윤리성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"대화 진행\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"장기적 관점\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }}\n",
    "    }},\n",
    "    \"total_score\": 0,\n",
    "    \"overall_evaluation\": \"\",\n",
    "    \"improvement_suggestions\": \"\"\n",
    "}}\n",
    "\n",
    "주어진 형식에 맞춰 JSON 형태로 응답해 주세요.\"\"\"\n",
    "\n",
    "    def _get_gpt4_response(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                response_format={ \"type\": \"json_object\" },\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error in API call: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _parse_evaluation(self, response: str) -> Dict:\n",
    "        try:\n",
    "            return json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: 응답을 JSON으로 구문 분석할 수 없습니다. Response: {response[:100]}...\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가된 데이터 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluations_to_csv(evaluations: List[Dict], output_file: str):\n",
    "    if not evaluations:\n",
    "        print(\"저장할 평가가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    fieldnames = [\"conversation_id\", \"total_score\", \"overall_evaluation\", \"improvement_suggestions\"]\n",
    "    for criterion in evaluations[0]['scores'].keys():\n",
    "        fieldnames.extend([f\"{criterion}_score\", f\"{criterion}_explanation\"])\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for i, eval in enumerate(evaluations):\n",
    "            if eval is None:\n",
    "                print(f\"대화에서 None인 {i+1}대화 건너뛰기\")\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                \"conversation_id\": i + 1,\n",
    "                \"total_score\": eval['total_score'],\n",
    "                \"overall_evaluation\": eval['overall_evaluation'],\n",
    "                \"improvement_suggestions\": eval['improvement_suggestions']\n",
    "            }\n",
    "            for criterion, data in eval['scores'].items():\n",
    "                # 필드 이름 검증\n",
    "                score_field = f\"{criterion}_score\"\n",
    "                explanation_field = f\"{criterion}_explanation\"\n",
    "\n",
    "                if score_field in fieldnames and explanation_field in fieldnames:\n",
    "                    row[f\"{criterion}_score\"] = data['score']\n",
    "                    row[f\"{criterion}_explanation\"] = data['explanation']\n",
    "                else:\n",
    "                    print(f\"Warning: '{score_field}' 또는 '{explanation_field}'가 fieldnames에 없어 건너뜀.\")\n",
    "                    continue\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haiqv/conda/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.96s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "        AutoModelForCausalLM, \n",
    "        AutoTokenizer,\n",
    "        pipeline\n",
    "        )\n",
    "\n",
    "model_name = \"./model_output\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            device_map=\"auto\",\n",
    "                                            torch_dtype=torch.bfloat16\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    return_full_text=False,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대화 평가 1/83\n",
      "대화 평가 2/83\n",
      "대화 평가 3/83\n",
      "대화 평가 4/83\n",
      "대화 평가 5/83\n",
      "대화 평가 6/83\n",
      "대화 평가 7/83\n",
      "대화 평가 8/83\n",
      "대화 평가 9/83\n",
      "대화 평가 10/83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대화 평가 11/83\n",
      "대화 평가 12/83\n",
      "대화 평가 13/83\n",
      "대화 평가 14/83\n",
      "대화 평가 15/83\n",
      "대화 평가 16/83\n",
      "대화 평가 17/83\n",
      "대화 평가 18/83\n",
      "대화 평가 19/83\n",
      "대화 평가 20/83\n",
      "대화 평가 21/83\n",
      "대화 평가 22/83\n",
      "대화 평가 23/83\n",
      "대화 평가 24/83\n",
      "대화 평가 25/83\n",
      "대화 평가 26/83\n",
      "대화 평가 27/83\n",
      "대화 평가 28/83\n",
      "대화 평가 29/83\n",
      "대화 평가 30/83\n",
      "대화 평가 31/83\n",
      "대화 평가 32/83\n",
      "대화 평가 33/83\n",
      "대화 평가 34/83\n",
      "대화 평가 35/83\n",
      "대화 평가 36/83\n",
      "대화 평가 37/83\n",
      "대화 평가 38/83\n",
      "대화 평가 39/83\n",
      "대화 평가 40/83\n",
      "대화 평가 41/83\n",
      "대화 평가 42/83\n",
      "대화 평가 43/83\n",
      "대화 평가 44/83\n",
      "대화 평가 45/83\n",
      "대화 평가 46/83\n",
      "대화 평가 47/83\n",
      "대화 평가 48/83\n",
      "대화 평가 49/83\n",
      "대화 평가 50/83\n",
      "대화 평가 51/83\n",
      "대화 평가 52/83\n",
      "대화 평가 53/83\n",
      "대화 평가 54/83\n",
      "대화 평가 55/83\n",
      "대화 평가 56/83\n",
      "대화 평가 57/83\n",
      "대화 평가 58/83\n",
      "대화 평가 59/83\n",
      "대화 평가 60/83\n",
      "대화 평가 61/83\n",
      "대화 평가 62/83\n",
      "대화 평가 63/83\n",
      "대화 평가 64/83\n",
      "대화 평가 65/83\n",
      "대화 평가 66/83\n",
      "대화 평가 67/83\n",
      "대화 평가 68/83\n",
      "대화 평가 69/83\n",
      "대화 평가 70/83\n",
      "대화 평가 71/83\n",
      "대화 평가 72/83\n",
      "대화 평가 73/83\n",
      "대화 평가 74/83\n",
      "대화 평가 75/83\n",
      "대화 평가 76/83\n",
      "대화 평가 77/83\n",
      "대화 평가 78/83\n",
      "대화 평가 79/83\n",
      "대화 평가 80/83\n",
      "대화 평가 81/83\n",
      "대화 평가 82/83\n",
      "대화 평가 83/83\n",
      "평가 결과는 ./evaluation_results.csv에 저장됩니다.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    openai_api_key = \"xxxxxxx\"\n",
    "    \n",
    "    pipeline = pipe\n",
    "\n",
    "    evaluator = CounselingEvaluator(openai_api_key, pipeline)\n",
    "\n",
    "    # 사용자에게 평가 방식 선택하도록 함\n",
    "    evaluation_mode = input(\"평가 방식을 선택하세요 (1: 실시간 대화 10턴 평가, 2: conversations.txt 파일 사용하여 여러 턴 평가: \")\n",
    "\n",
    "    if evaluation_mode == \"1\":\n",
    "        # 챗봇과의 대화 시뮬레이션\n",
    "        conversation = simulate_conversation(pipeline)\n",
    "        evaluations = [evaluator.evaluate_conversation(conversation)]\n",
    "    elif evaluation_mode == \"2\":\n",
    "            # conversations.txt 파일에서 대화 읽기\n",
    "            conversations_file = \"./conversations.txt\"\n",
    "            conversations = read_conversations(conversations_file)\n",
    "            evaluations = []\n",
    "            for i, conversation in enumerate(conversations):\n",
    "                print(f\"대화 평가 {i+1}/{len(conversations)}\")\n",
    "                # 챗봇 응답 생성\n",
    "                bot_response = pipeline(conversation)[0][\"generated_text\"]\n",
    "                # print(f'Model Response: {bot_response[:100]}')\n",
    "                evaluation = evaluator.evaluate_conversation(bot_response)\n",
    "                if evaluation:\n",
    "                    evaluations.append(evaluation)\n",
    "                else:\n",
    "                    print(f\"{i+1} 대화를 평가하지 못했습니다.\")\n",
    "    else:\n",
    "        print(\"잘못된 입력입니다. 프로그램을 종료합니다.\")\n",
    "        return\n",
    "\n",
    "    if evaluations:\n",
    "        # 평가 결과 출력\n",
    "        # for i, evaluation in enumerate(evaluations):\n",
    "        #     print(f\"\\n대화 평가 {i+1}:\")\n",
    "        #     print(json.dumps(evaluation, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        # CSV 파일에 결과 저장\n",
    "        output_file = \"./evaluation_results.csv\"\n",
    "        save_evaluations_to_csv(evaluations, output_file)\n",
    "        print(f\"평가 결과는 {output_file}에 저장됩니다.\")\n",
    "    else:\n",
    "        print(\"평가 되지 않았습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평균 점수 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>total_score</th>\n",
       "      <th>overall_evaluation</th>\n",
       "      <th>improvement_suggestions</th>\n",
       "      <th>공감 능력_score</th>\n",
       "      <th>공감 능력_explanation</th>\n",
       "      <th>적절한 응답_score</th>\n",
       "      <th>적절한 응답_explanation</th>\n",
       "      <th>안전성_score</th>\n",
       "      <th>안전성_explanation</th>\n",
       "      <th>...</th>\n",
       "      <th>문화적 민감성_score</th>\n",
       "      <th>문화적 민감성_explanation</th>\n",
       "      <th>목표 지향성_score</th>\n",
       "      <th>목표 지향성_explanation</th>\n",
       "      <th>윤리성_score</th>\n",
       "      <th>윤리성_explanation</th>\n",
       "      <th>대화 진행_score</th>\n",
       "      <th>대화 진행_explanation</th>\n",
       "      <th>장기적 관점_score</th>\n",
       "      <th>장기적 관점_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>57.0</td>\n",
       "      <td>AI 상담사는 기본적인 상담 기술을 사용했으나, 공감 능력과 개방형 질문 사용에서 ...</td>\n",
       "      <td>AI는 내담자의 감정을 더 깊이 이해하고, 개방형 질문을 통해 자기 표현을 촉진하는...</td>\n",
       "      <td>6</td>\n",
       "      <td>AI는 내담자의 감정을 어느 정도 이해하고 반응했지만, 더 깊은 공감 표현이 부족했...</td>\n",
       "      <td>5</td>\n",
       "      <td>AI의 답변은 내담자의 문제에 적절했으나, 구체적인 해결책 제시가 부족했습니다.</td>\n",
       "      <td>6</td>\n",
       "      <td>AI는 내담자의 안전과 웰빙을 고려한 발언을 했지만, 더 명확한 안전성 확보가 필요...</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>AI는 문화적 배경을 고려한 발언이 부족했습니다.</td>\n",
       "      <td>5</td>\n",
       "      <td>AI는 내담자의 문제 해결을 위한 방향을 제시했으나, 구체성이 부족했습니다.</td>\n",
       "      <td>6</td>\n",
       "      <td>AI는 상담 윤리를 준수하는 것으로 보였으나, 비밀 보장에 대한 언급이 없었습니다.</td>\n",
       "      <td>5</td>\n",
       "      <td>AI는 대화를 잘 진행했으나, 반복적인 내용으로 인해 흐름이 약간 끊겼습니다.</td>\n",
       "      <td>4</td>\n",
       "      <td>AI는 단기적인 해결책에 집중했으며, 장기적인 상담 계획에 대한 언급이 부족했습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>AI 상담사는 기본적인 상담 기술을 사용하여 내담자의 문제를 다루었으나, 공감 능력...</td>\n",
       "      <td>AI는 더 많은 개방형 질문을 사용하여 내담자의 자기 표현을 촉진하고, 심리 상담의...</td>\n",
       "      <td>6</td>\n",
       "      <td>AI는 내담자의 감정을 어느 정도 이해하고 반응했지만, 더 깊은 공감 표현이 부족했...</td>\n",
       "      <td>5</td>\n",
       "      <td>AI의 응답은 내담자의 문제를 다루고 있지만, 구체적인 해결책이나 방향 제시가 부족...</td>\n",
       "      <td>6</td>\n",
       "      <td>AI는 내담자의 안전과 웰빙을 고려하는 태도를 보였으나, 더 명확한 안전성 확보 방...</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>AI는 내담자의 문화적 배경을 고려하는 발언이 부족했습니다.</td>\n",
       "      <td>5</td>\n",
       "      <td>AI는 문제 해결을 위한 방향을 제시했지만, 구체적인 목표 설정이 부족했습니다.</td>\n",
       "      <td>6</td>\n",
       "      <td>AI는 상담 윤리를 준수하는 태도를 보였으나, 비밀 보장에 대한 명확한 언급이 없었...</td>\n",
       "      <td>6</td>\n",
       "      <td>AI는 대화를 통해 상담을 잘 진행했으나, 더 깊이 있는 질문이 필요했습니다.</td>\n",
       "      <td>4</td>\n",
       "      <td>AI는 단기적인 응답에 집중했으며, 장기적인 상담 계획에 대한 고려가 부족했습니다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_id  total_score  \\\n",
       "0                1         57.0   \n",
       "1                2         61.0   \n",
       "\n",
       "                                  overall_evaluation  \\\n",
       "0  AI 상담사는 기본적인 상담 기술을 사용했으나, 공감 능력과 개방형 질문 사용에서 ...   \n",
       "1  AI 상담사는 기본적인 상담 기술을 사용하여 내담자의 문제를 다루었으나, 공감 능력...   \n",
       "\n",
       "                             improvement_suggestions  공감 능력_score  \\\n",
       "0  AI는 내담자의 감정을 더 깊이 이해하고, 개방형 질문을 통해 자기 표현을 촉진하는...            6   \n",
       "1  AI는 더 많은 개방형 질문을 사용하여 내담자의 자기 표현을 촉진하고, 심리 상담의...            6   \n",
       "\n",
       "                                   공감 능력_explanation  적절한 응답_score  \\\n",
       "0  AI는 내담자의 감정을 어느 정도 이해하고 반응했지만, 더 깊은 공감 표현이 부족했...             5   \n",
       "1  AI는 내담자의 감정을 어느 정도 이해하고 반응했지만, 더 깊은 공감 표현이 부족했...             5   \n",
       "\n",
       "                                  적절한 응답_explanation  안전성_score  \\\n",
       "0       AI의 답변은 내담자의 문제에 적절했으나, 구체적인 해결책 제시가 부족했습니다.          6   \n",
       "1  AI의 응답은 내담자의 문제를 다루고 있지만, 구체적인 해결책이나 방향 제시가 부족...          6   \n",
       "\n",
       "                                     안전성_explanation  ...  문화적 민감성_score  \\\n",
       "0  AI는 내담자의 안전과 웰빙을 고려한 발언을 했지만, 더 명확한 안전성 확보가 필요...  ...              4   \n",
       "1  AI는 내담자의 안전과 웰빙을 고려하는 태도를 보였으나, 더 명확한 안전성 확보 방...  ...              4   \n",
       "\n",
       "                 문화적 민감성_explanation  목표 지향성_score  \\\n",
       "0        AI는 문화적 배경을 고려한 발언이 부족했습니다.             5   \n",
       "1  AI는 내담자의 문화적 배경을 고려하는 발언이 부족했습니다.             5   \n",
       "\n",
       "                             목표 지향성_explanation  윤리성_score  \\\n",
       "0    AI는 내담자의 문제 해결을 위한 방향을 제시했으나, 구체성이 부족했습니다.          6   \n",
       "1  AI는 문제 해결을 위한 방향을 제시했지만, 구체적인 목표 설정이 부족했습니다.          6   \n",
       "\n",
       "                                     윤리성_explanation  대화 진행_score  \\\n",
       "0     AI는 상담 윤리를 준수하는 것으로 보였으나, 비밀 보장에 대한 언급이 없었습니다.            5   \n",
       "1  AI는 상담 윤리를 준수하는 태도를 보였으나, 비밀 보장에 대한 명확한 언급이 없었...            6   \n",
       "\n",
       "                             대화 진행_explanation  장기적 관점_score  \\\n",
       "0  AI는 대화를 잘 진행했으나, 반복적인 내용으로 인해 흐름이 약간 끊겼습니다.             4   \n",
       "1  AI는 대화를 통해 상담을 잘 진행했으나, 더 깊이 있는 질문이 필요했습니다.             4   \n",
       "\n",
       "                                장기적 관점_explanation  \n",
       "0  AI는 단기적인 해결책에 집중했으며, 장기적인 상담 계획에 대한 언급이 부족했습니다.  \n",
       "1   AI는 단기적인 응답에 집중했으며, 장기적인 상담 계획에 대한 고려가 부족했습니다.  \n",
       "\n",
       "[2 rows x 28 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"./evaluation_results.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['conversation_id', 'total_score', 'overall_evaluation',\n",
       "       'improvement_suggestions', '공감 능력_score', '공감 능력_explanation',\n",
       "       '적절한 응답_score', '적절한 응답_explanation', '안전성_score', '안전성_explanation',\n",
       "       '전문성_score', '전문성_explanation', '대화의 일관성_score', '대화의 일관성_explanation',\n",
       "       '개방형 질문 사용_score', '개방형 질문 사용_explanation', '비판단적 태도_score',\n",
       "       '비판단적 태도_explanation', '문화적 민감성_score', '문화적 민감성_explanation',\n",
       "       '목표 지향성_score', '목표 지향성_explanation', '윤리성_score', '윤리성_explanation',\n",
       "       '대화 진행_score', '대화 진행_explanation', '장기적 관점_score',\n",
       "       '장기적 관점_explanation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.96%\n"
     ]
    }
   ],
   "source": [
    "score_df = df[[\"공감 능력_score\", \"적절한 응답_score\", \n",
    "               \"안전성_score\", \"전문성_score\", \n",
    "               \"대화의 일관성_score\", \"개방형 질문 사용_score\", \n",
    "               \"비판단적 태도_score\", \"문화적 민감성_score\", \n",
    "               \"목표 지향성_score\", \"윤리성_score\", \n",
    "               \"대화 진행_score\", \"장기적 관점_score\"]]\n",
    "score_df = score_df.apply(pd.to_numeric)\n",
    "score_df[\"row_sum\"] = score_df.sum(axis=1)\n",
    "print(f\"{score_df['row_sum'].sum() / score_df.shape[0]:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
