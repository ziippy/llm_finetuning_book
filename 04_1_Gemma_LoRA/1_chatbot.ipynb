{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma-2-9B-it 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haiqv/conda/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import (setup_chat_format, \n",
    "                 DataCollatorForCompletionOnlyLM, \n",
    "                 SFTTrainer)\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, PeftConfig \n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForCausalLM, \n",
    "                          TrainingArguments, \n",
    "                          BitsAndBytesConfig, \n",
    "                          pipeline, \n",
    "                          StoppingCriteria)\n",
    "\n",
    "model_id = \"google/gemma-2-9b-it\" \n",
    "\n",
    "# 모델과 토크나이저 불러오기 \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='eager'\n",
    "    # load_in_8bit=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-02 14:07:34--  https://raw.githubusercontent.com/MrBananaHuman/CounselGPT/main/total_kor_multiturn_counsel_bot.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30560672 (29M) [application/octet-stream]\n",
      "Saving to: ‘total_kor_multiturn_counsel_bot.jsonl’\n",
      "\n",
      "total_kor_multiturn 100%[===================>]  29.14M   109MB/s    in 0.3s    \n",
      "\n",
      "2025-01-02 14:07:36 (109 MB/s) - ‘total_kor_multiturn_counsel_bot.jsonl’ saved [30560672/30560672]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/MrBananaHuman/CounselGPT/main/total_kor_multiturn_counsel_bot.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./total_kor_multiturn_counsel_bot.jsonl', \n",
    "          'r', \n",
    "          encoding='utf-8') as file:\n",
    "      original_jsonl_data = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'speaker': '상담사', 'utterance': '안녕하세요. 심리상담사입니다. 어떤 고민이 있으신가요?'},\n",
       " {'speaker': '내담자', 'utterance': '요즘 직장에서 너무 힘들어요.'},\n",
       " {'speaker': '상담사', 'utterance': '정말요? 어떤 점이 힘드신가요? 좀 더 자세히 말해주세요.'},\n",
       " {'speaker': '내담자',\n",
       "  'utterance': '친한 동료도 없고 일이 너무 많고 고객이나 동료에게 매일 반응하고 대처해야하니까 점점 지쳐 가네요.'},\n",
       " {'speaker': '상담사',\n",
       "  'utterance': '그러셨군요. 직장생활에서 하나하나 대응하는 일은 많은 에너지를 필요로 합니다. 그리고 이러한 에너지 소모는 급격히 힘들어지게 합니다. 이러한 일상에 적응하며 시간이 지나면 점점 힘들어질 수 있어요.'},\n",
       " {'speaker': '내담자', 'utterance': '집에 가면 집안일을 하고 나면 무언가를 해야하는데 그게 너무 힘들어요.'},\n",
       " {'speaker': '상담사',\n",
       "  'utterance': '집에서도 일을 하시는군요. 그러시다보니 집에서의 일도 의무적으로 느껴지는 거 같아요. 이러한 의무감에 의해서 불안감과 힘들어질 수 있죠.'},\n",
       " {'speaker': '내담자', 'utterance': '이러다 몸이 아플 것 같아요. 이게 계속되면 어떻게 해야할까요?'},\n",
       " {'speaker': '상담사',\n",
       "  'utterance': '몸이 힘들어지는 건 자신이 지니고 있는 신호입니다. 즉, 몸과 마음에 신호를 주고 있는 거죠. 혹시 이러한 증상이 지속되시면 주변의 내용을 통해 주변의 상황을 살펴보고, 다양한 자신의 취미를 발견하거나, 휴식을 통해서 쉬는 것도 좋습니다. 만약에 몸에 이상을 느끼신다면 병원에 찾아가셔서 다양한 건강상의 문제를 예방할 수 있도록 조치하세요.'},\n",
       " {'speaker': '상담사', 'utterance': '내담자님, 어떤 생각이 드시나요?'},\n",
       " {'speaker': '내담자', 'utterance': '생각을 잘 못해서요.'},\n",
       " {'speaker': '상담사',\n",
       "  'utterance': '그러시면, 우선 이러한 일상에 대해서 고민해보세요. 머리를 비우고 쉬어도 좋고, 진지하게 자신의 일상을 돌아보면서 어떻게 하면 이러한 고민을 줄일 수 있는지 생각해보세요.'},\n",
       " {'speaker': '상담사', 'utterance': '어떤 생각을 하셨나요?'},\n",
       " {'speaker': '내담자', 'utterance': '가족이랑 시간을 보내면서 즐겁게 생활해야겠다는 생각이 들었어요.'},\n",
       " {'speaker': '상담사',\n",
       "  'utterance': '그렇군요. 가족이나 친구와의 소통은 그만큼의 만족감과 편안함을 가져다줄 수 있죠. 다양한 시간과 경험을 나누면서 그 사람들과 더 가까워질 수 있을 거 같아요.'},\n",
       " {'speaker': '상담사', 'utterance': '더 말씀하실 내용이 있으신가요?'},\n",
       " {'speaker': '내담자', 'utterance': '없어요. 감사합니다.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_jsonl_data[5085]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내담자를 user 로\n",
    "상담사를 assistant 로 변환\n",
    "\n",
    "assistant 의 발화로 대화가 시작하는 경우 -> 해당 첫 메시지를 삭제해 -> user 의 질문으로 시작하게 조정  \n",
    "user 의 발화로 끝나는 경우 -> 마지막 메시지를 삭제해 -> assistant 의 응답으로 끝나게 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_dict = {'내담자': 'user', '상담사': 'assistant'}\n",
    "\n",
    "def preprocess_conversation(messages):\n",
    "    # speaker를 role로 변환\n",
    "    converted_messages = [{'role': speaker_dict[m['speaker']], 'content': m['utterance']} for m in messages]\n",
    "    \n",
    "    # assistant로 시작하는 경우 첫 메시지 제거\n",
    "    if converted_messages and converted_messages[0]['role'] == 'assistant':\n",
    "        converted_messages = converted_messages[1:]\n",
    "    \n",
    "    # user로 끝나는 경우 마지막 메시지들 제거\n",
    "    while converted_messages and converted_messages[-1]['role'] == 'user':\n",
    "        converted_messages = converted_messages[:-1]\n",
    "    \n",
    "    # 연속된 동일 역할의 메시지 병합\n",
    "    converted_messages = merge_consecutive_messages(converted_messages)\n",
    "    \n",
    "    # 대화가 비어있거나 홀수 개의 메시지만 남은 경우 처리\n",
    "    if not converted_messages or len(converted_messages) % 2 != 0:\n",
    "        return []\n",
    "    \n",
    "    return converted_messages\n",
    "\n",
    "def merge_consecutive_messages(messages):\n",
    "    if not messages:\n",
    "        return []\n",
    "    \n",
    "    merged = []\n",
    "    current_role = messages[0]['role']\n",
    "    current_content = messages[0]['content']\n",
    "    \n",
    "    for message in messages[1:]:\n",
    "        if message['role'] == current_role:\n",
    "            current_content += \" \" + message['content']\n",
    "        else:\n",
    "            merged.append({'role': current_role, 'content': current_content})\n",
    "            current_role = message['role']\n",
    "            current_content = message['content']\n",
    "    \n",
    "    merged.append({'role': current_role, 'content': current_content})\n",
    "    return merged\n",
    "\n",
    "\n",
    "def transform_to_new_format(original_data):\n",
    "    transformed_data = []\n",
    "    for conversation in original_data:\n",
    "        processed_conversation = preprocess_conversation(conversation)\n",
    "        if processed_conversation:\n",
    "            transformed_data.append(processed_conversation)\n",
    "    return transformed_data\n",
    "\n",
    "result = transform_to_new_format(original_jsonl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': '내가 약간 중2병 같은 걸 증상을 보이고 있어요.'},\n",
       " {'role': 'assistant', 'content': '중2병 증상이라니, 어떤 증상이신 건가요?'},\n",
       " {'role': 'user',\n",
       "  'content': '그러니까 공부하기 싫어하고, 공격적이고, 좀 무례하게 말하고 싶은 게 많아져서 그런 거예요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '그런 증상이 있으니까 힘드시겠죠. 중2병 같은 것이라고 생각하시는 이유는 무엇인가요?'},\n",
       " {'role': 'user', 'content': '막 공부 안하고 이것저것 들먹이고 하고 싶은 게 너무 많아서 그런 거 같아요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '그런 것도 어쩔 수 없이 찾아오는 시기가 있으니까 무리하지 않도록 해야겠죠. 대학교를 가면서 나아질 것 같았는데, 오히려 더 심해진 것 같다고 하셨죠. 그 원인이 무엇인가요?'},\n",
       " {'role': 'user', 'content': '그걸 제가 잘 몰라서 그런 것 같아요. 그냥 더 심해졌다고 느꼈어요.'},\n",
       " {'role': 'assistant', 'content': '대학교 생활이 신나고 재밌으신 건 어떤 점이 있나요?'},\n",
       " {'role': 'user',\n",
       "  'content': '학과가 정말 좋아서 즐겁게 수업을 듣고 있어요. 학우들도 좋고 괜찮은 친구들도 많이 만나서 그런 것 같아요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '즐거운 일도 많이 있으면서 고민거리도 있는 것 같군요. 가사나 소설을 쓰시면서 마음을 풀기도 하신다고 하셨는데, 언제부터 그 습관이 생겨난 건가요?'},\n",
       " {'role': 'user',\n",
       "  'content': '좋은 질문이에요. 좀 자세히 말씀드릴게요. 학교에서 어려운 일이 있었는데, 그 때부터 가사나 소설 같은 것들을 쓰면서 마음을 풀게 되었어요. 그리고 이런 걸 쓰면서 나름 살아갈 때도 있는 것 같고, 그러다 보니까 늘어나는 것 같아요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '어려운 일이 있으셨군요. 그 때부터 쓰시면서 나아지는 기분이 드셨다고 하셨는데, 현재에도 그런 기분이 드시나요?'},\n",
       " {'role': 'user',\n",
       "  'content': '좀 그래요. 이제는 어느 정도 극복했다고 생각은 하지만, 가사나 소설을 쓰면서 마음이 편안해지는 느낌이 있어서 써왔어요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '가사나 소설 같은 것들이 나쁜 것은 아니라고 하셨죠. 하지만, 과도한 습관으로 만들어지면 나중에 문제가 생길 수도 있습니다. 이러한 습관을 줄이고, 보다 효율적으로 자아확인에 도움이 될 수 있도록 일정한 패턴과 기록 방식으로 습관화하는 것이 좋습니다. 예를 들어, 가사나 소설을 쓸 때 일주일에 몇 번, 어느 시간대에 쓰는 것이 효과적인지에 대해 스스로에게 질문해보고, 그에 따른 습관을 만들어보는 것도 좋은 방법입니다.'},\n",
       " {'role': 'user', 'content': '그럼, 가사나 소설 같은 것들을 과도하게 하면 나중에 문제가 생긴다는 건가요?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '그렇습니다. 이러한 습관이 과도하게 되면 자신에게 과부하를 주고 심리적인 문제를 일으킬 수 있습니다. 때로는, 삶의 태도를 바꾸는 것도 중요합니다. 스스로를 사랑하고 용서하며 자기자신에게 긍정적인 자아이미지를 가지는 것도 중요합니다. 이러한 자아이미지가 긍정적일수록 스트레스를 받아도 잘 해결할 수 있고, 자기자신을 존중하고 이해하는 능력이 생깁니다.'},\n",
       " {'role': 'user', 'content': '그래도 좀 걱정이 되네요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '걱정은 당연한 것이죠. 그러나 지나치게 걱정하는 것은 오히려 스트레스를 더 받게 됩니다. 지금 이 곳에 오셔서 이야기를 나누어 봤을 때, 내담자님께서 가진 문제가 그렇게 심각한 것은 아닌 것 같습니다. 하지만, 무엇보다도, 내담자님이 존중하고 사랑하는 마음으로 자신을 바라보며, 나름의 방식으로 살아가는 것이 중요합니다. 무언가를 끼워 맞추어 주려 하지 마시고, 스스로 찾아낼 수 있는 길을 찾아보세요.'},\n",
       " {'role': 'user', 'content': '그 말씀대로 할게요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '그렇게 하셔서 조금이나마 좋아지시길 바라겠습니다. 이후에도 힘든 마음이 계속되면 언제든지 저를 찾아주세요.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./train_dataset.jsonl\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for conversation in result:\n",
    "        json_obj = {\"messages\": conversation}\n",
    "        json.dump(json_obj, file, ensure_ascii=False)\n",
    "        file.write(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 8731 examples [00:00, 153760.27 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 8731\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./train_dataset.jsonl\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 8731\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./train_dataset.jsonl\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': '요즘 직장에서 너무 힘들어요.'},\n",
       " {'role': 'assistant', 'content': '정말요? 어떤 점이 힘드신가요? 좀 더 자세히 말해주세요.'},\n",
       " {'role': 'user',\n",
       "  'content': '친한 동료도 없고 일이 너무 많고 고객이나 동료에게 매일 반응하고 대처해야하니까 점점 지쳐 가네요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '그러셨군요. 직장생활에서 하나하나 대응하는 일은 많은 에너지를 필요로 합니다. 그리고 이러한 에너지 소모는 급격히 힘들어지게 합니다. 이러한 일상에 적응하며 시간이 지나면 점점 힘들어질 수 있어요.'},\n",
       " {'role': 'user', 'content': '집에 가면 집안일을 하고 나면 무언가를 해야하는데 그게 너무 힘들어요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '집에서도 일을 하시는군요. 그러시다보니 집에서의 일도 의무적으로 느껴지는 거 같아요. 이러한 의무감에 의해서 불안감과 힘들어질 수 있죠.'},\n",
       " {'role': 'user', 'content': '이러다 몸이 아플 것 같아요. 이게 계속되면 어떻게 해야할까요?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '몸이 힘들어지는 건 자신이 지니고 있는 신호입니다. 즉, 몸과 마음에 신호를 주고 있는 거죠. 혹시 이러한 증상이 지속되시면 주변의 내용을 통해 주변의 상황을 살펴보고, 다양한 자신의 취미를 발견하거나, 휴식을 통해서 쉬는 것도 좋습니다. 만약에 몸에 이상을 느끼신다면 병원에 찾아가셔서 다양한 건강상의 문제를 예방할 수 있도록 조치하세요. 내담자님, 어떤 생각이 드시나요?'},\n",
       " {'role': 'user', 'content': '생각을 잘 못해서요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '그러시면, 우선 이러한 일상에 대해서 고민해보세요. 머리를 비우고 쉬어도 좋고, 진지하게 자신의 일상을 돌아보면서 어떻게 하면 이러한 고민을 줄일 수 있는지 생각해보세요. 어떤 생각을 하셨나요?'},\n",
       " {'role': 'user', 'content': '가족이랑 시간을 보내면서 즐겁게 생활해야겠다는 생각이 들었어요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '그렇군요. 가족이나 친구와의 소통은 그만큼의 만족감과 편안함을 가져다줄 수 있죠. 다양한 시간과 경험을 나누면서 그 사람들과 더 가까워질 수 있을 거 같아요. 더 말씀하실 내용이 있으신가요?'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"messages\"][5085]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"up_proj\",\n",
    "            \"o_proj\",\n",
    "            \"k_proj\",\n",
    "            \"down_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./model_output\", \n",
    "    num_train_epochs=1,          \n",
    "    # per_device_train_batch_size=2,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,  \n",
    "    optim=\"adamw_torch_fused\",    \n",
    "    logging_steps=100,            \n",
    "    save_strategy=\"epoch\",        \n",
    "    learning_rate=2e-4,           \n",
    "    bf16=True,                    \n",
    "    tf32=True,                    \n",
    "    max_grad_norm=0.3,            \n",
    "    warmup_ratio=0.03,            \n",
    "    lr_scheduler_type=\"constant\", \n",
    "    push_to_hub=True,             \n",
    "    # report_to=\"wandb\",            \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haiqv/conda/envs/py310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/haiqv/conda/envs/py310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:323: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=512,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1725' max='1725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1725/1725 1:24:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.295300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.272600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.273700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.252400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.248200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1725, training_loss=1.2754829605765965, metrics={'train_runtime': 5057.7481, 'train_samples_per_second': 2.728, 'train_steps_per_second': 0.341, 'total_flos': 3.895332015439872e+17, 'train_loss': 1.2754829605765965, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습한 모델 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haiqv/conda/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요즘 힘이 드네요.\n",
      "model\n",
      "네, 그러셨군요. 힘들어 보이시는데, 무엇이 힘드셨나요?\n",
      "user\n",
      "요즘 기분이 너무 울적하고, 집에 가면 밥을 먹고 잠자고 일어나서 다시 학교나 회사를 가야 해서 계속해서 지치고 힘들어요. 그리고 다른 사람들과 대화를 나누는 것도 어렵고, 집에 있고 싶어요. 어떻게 하면 좀 낫게 될까요?\n",
      "model\n",
      "힘들어 보이시네요. 기분이 울적하다는 거도 힘든 일이죠. 무엇 때문에 그런 기분이 드는 건가요?\n",
      "user\n",
      "그건 잘 모르겠어요.\n",
      "model\n",
      "그렇다면, 좀 더 자세히 말씀해주실 수 있나요?\n",
      "user\n",
      "그냥, 힘들어서 그런 것 같아요.\n",
      "model\n",
      "그렇군요. 혹시 이런 증상이 언제부터 시작된 것인가요?\n",
      "user\n",
      "그건 좀 전부터 그러더라고요.\n",
      "model\n",
      "전부터 이런 증상이 있었군요. 그렇다면 이전에는 괜찮았을 때는 이런 증상이 없었을 거겠죠. 그렇다면 이전에는 어떻게 해결하셨나요?\n",
      "user\n",
      "그냥 시간이 지나고 괜찮아졌어요.\n",
      "model\n",
      "그러셨군요. 이전에도 그랬던 적도 있었겠지만, 괜찮아졌다는 것이 중요해요. 이번에도 괜\n"
     ]
    }
   ],
   "source": [
    "### generate 이용해서 테스트\n",
    "import torch\n",
    "from transformers import (\n",
    "        AutoModelForCausalLM, \n",
    "        AutoTokenizer\n",
    "        )\n",
    "\n",
    "model_name = \"./model_output\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            device_map=\"auto\",\n",
    "                                            torch_dtype=torch.bfloat16\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 입력 텍스트를 토큰화합니다\n",
    "input_text = \"요즘 힘이 드네\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 텍스트를 생성합니다\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=400,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# 생성된 텍스트를 디코딩합니다\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요즘 힘이 드네요.\n",
      "model\n",
      "그렇게 느끼시다니, 이유가 있으신가요?\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "# 이 프로젝트는 챗봇 대화를 위한 것이므로 사용자가 말하는 부분은 생성되지 않아야 한다.\n",
    "# 즉, 'user' 라는 단어가 생성되면 텍스트 생성을 중단해야 한다.\n",
    "# 이를 위해 StoppingCriteria 를 설정한다.\n",
    "import torch\n",
    "from transformers import (\n",
    "        AutoModelForCausalLM, \n",
    "        AutoTokenizer, \n",
    "        StoppingCriteria, \n",
    "        StoppingCriteriaList\n",
    "        )\n",
    "\n",
    "model_name = \"./model_output\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            device_map=\"auto\",\n",
    "                                            torch_dtype=torch.bfloat16\n",
    "                                            )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 'user' 토큰의 ID를 찾습니다\n",
    "user_token_id = tokenizer.encode(\"user\", add_special_tokens=False)[0]\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, stop_token_ids):\n",
    "        super().__init__()\n",
    "        self.stop_token_ids = stop_token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.stop_token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stop_words_ids = [user_token_id]\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens(stop_token_ids=stop_words_ids)])\n",
    "\n",
    "# 입력 텍스트를 토큰화합니다\n",
    "input_text = \"요즘 힘이 드네\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 텍스트를 생성합니다\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=400,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# 생성된 텍스트를 디코딩합니다\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline 을 이용한 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "        AutoModelForCausalLM, \n",
    "        AutoTokenizer,\n",
    "        pipeline\n",
    "        )\n",
    "\n",
    "model_name = \"./model_output\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            device_map=\"auto\",\n",
    "                                            torch_dtype=torch.bfloat16\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    return_full_text=False,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "내담자님, 그런 고민이 있으니 힘드실 거 같아요. 그동안 어떻게 해오셨나요?\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "# 입력 텍스트\n",
    "input_text = \"안녕하세요. 제가 강박증이 있는 것 같아요. 자꾸 문을 잠갔는지 확인하게 되고, 확인하지 않으면 불안해서 견딜 수가 없어요.\"\n",
    "\n",
    "# 텍스트 생성\n",
    "output = pipe(\n",
    "    input_text,\n",
    "    max_new_tokens=1000,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
